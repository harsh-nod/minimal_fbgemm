// !!! This is a file automatically generated by hipify!!!
#include "hip/hip_runtime.h"
////////////////////////////////////////////////////////////////////////////////
// GENERATED FILE INFO
//
// Template Source: training/backward/embedding_backward_split_indice_weights_template.cu
////////////////////////////////////////////////////////////////////////////////

#define __TEMPLATE_SOURCE_FILE__ "training/backward/embedding_backward_split_indice_weights_template.cu"

/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
////////////////////////////////////////////////////////////////////////////////
// Required for op registrations
////////////////////////////////////////////////////////////////////////////////
#include "fbgemm_gpu/embedding_forward_template_helpers_hip.cuh"
#include "fbgemm_gpu/utils/ops_utils.h"
#include "fbgemm_gpu/utils/tensor_utils.h"
#include "fbgemm_gpu/utils/assert_macros.h"
#include "fbgemm_gpu/utils/kernel_launcher_hip.cuh"
#include "fbgemm_gpu/rocm/cdna_guard.h"

using Tensor = at::Tensor;
using namespace fbgemm_gpu;

#define DISPATCH_NON_VEC_BLOCKING_KERNEL(MAX_D, ...) \
  [&] {                                              \
    if (MAX_D <= 256) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               1;                                      \
             constexpr int kFixedMaxVecsPerThread = 1; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 512) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               2;                                      \
             constexpr int kFixedMaxVecsPerThread = 2; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 768) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               3;                                      \
             constexpr int kFixedMaxVecsPerThread = 3; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1024) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               4;                                      \
             constexpr int kFixedMaxVecsPerThread = 4; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1280) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               5;                                      \
             constexpr int kFixedMaxVecsPerThread = 5; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        if (MAX_D <= 1536) {                               \
             [[ maybe_unused ]] const int max_vecs_per_thread =               \
               6;                                      \
             constexpr int kFixedMaxVecsPerThread = 6; \
             [[ maybe_unused ]] constexpr int kThreadGroupSize =              \
               64;                                            \
             [[ maybe_unused ]] constexpr bool kUseVecBlocking =              \
               false;                                             \
             return __VA_ARGS__();                                            \
           }                                                                  \
        }()

#define DISPATCH_VEC_BLOCKING_KERNEL(MAX_D, ...)     \
  [&] {                                              \
    if (MAX_D > 1536) {                                     \
         [[ maybe_unused ]] const int max_vecs_per_thread =                  \
           (MAX_D + 256 - 1) / 256;                \
         constexpr int kFixedMaxVecsPerThread = 6; \
         [[ maybe_unused ]] constexpr int kThreadGroupSize = kWarpSize;      \
         [[ maybe_unused ]] constexpr bool kUseVecBlocking = true;           \
         return __VA_ARGS__();                                               \
       }                                                                     \
    }()

// TODO: optimization to use multiple warps per row.
template <
  typename emb_t,
  typename grad_t,
  typename cache_t,
  typename index_t,
  int32_t kFixedMaxVecsPerThread,
  bool embDimMatch
>
__global__ __launch_bounds__(kForwardMaxThreads) void
split_embedding_codegen_grad_indice_weights_vbe_vec_blocking_kernel(
    // [\sum_t E_t x D_t]
    const pta::PackedTensorAccessor64<grad_t, 2, at::RestrictPtrTraits> grad_output,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> dev_weights,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> uvm_weights,
    pta::PackedTensorAccessor64<cache_t, 2, at::RestrictPtrTraits> lxu_cache_weights,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> weights_placements,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> weights_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> D_offsets,
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> indices, // [N = \sum_{b,t} L_{b,t} total indices, i.e. flattened [B][T][L]
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> offsets, // [B x T + 1]
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> lxu_cache_locations,
    pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> feature_requires_grad, // [T],
    pta::PackedTensorAccessor32<at::acc_type<cache_t, true>, 1, at::RestrictPtrTraits> grad_indice_weights,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> row_grad_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> b_t_map,
    const int32_t info_B_num_bits,
    const uint32_t info_B_mask
    ) {
    constexpr int32_t kVecWidth = 4;
    int error_code = 0;
    int64_t error_value;

    int32_t T = D_offsets.size(0) - 1;
    auto b_t = blockIdx.x * blockDim.y + threadIdx.y;
    if (b_t >= offsets.size(0) - 1) {
        return;
    }

    int32_t t;
    [[maybe_unused]] int32_t b;
    const auto info = reinterpret_cast<const uint32_t*>(&b_t_map[b_t])[0];
    reinterpret_cast<uint32_t*>(&t)[0] = info >> info_B_num_bits;
    reinterpret_cast<uint32_t*>(&b)[0] = info & info_B_mask;

    const auto weights_offset = weights_offsets[t];
    const auto D_start = D_offsets[t];
    const auto D_end = D_offsets[t + 1];
    const auto D = D_end - D_start;
    auto D_emb = D;
    if constexpr (std::is_same_v<emb_t, uint8_t>) {
      D_emb += kINT8QparamsBytes;
    }
    const auto indices_start = offsets[b_t];
    const auto indices_end = offsets[b_t + 1];
    const auto L = indices_end - indices_start;
    if (feature_requires_grad.size(0) > 0 && !feature_requires_grad[t]) {
        // If the table does not require gradient computation, we set the gradient to zero.
        for (auto l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            if (l < L) {
                grad_indice_weights[indices_start + l] = 0.0;
            }
        }
        return;
    }

    emb_t* __restrict__ weights;
    overflow_safe_int_t weights_numel;
    const auto placement = static_cast<PlacementType>(weights_placements[t]);
    if (placement == PlacementType::DEVICE) {
        weights = &dev_weights[weights_offset];
        weights_numel = dev_weights.size(0) - weights_offset;
    } else {
        weights = &uvm_weights[weights_offset];
        weights_numel = uvm_weights.size(0) - weights_offset;
    }
    const grad_t* grad_output_ = &grad_output[0][row_grad_offsets[b_t]];

    Vec4TAcc<cache_t> grad_out[kFixedMaxVecsPerThread];
    const int32_t num_vecs = div_round_up(D, kWarpSize * kVecWidth);
    for (int32_t vec_start = 0;
         vec_start < num_vecs;
         vec_start += kFixedMaxVecsPerThread) { 

        // Load gradients
        // TODO: Maybe using a combination of shared memory and registers is
        // better for performance
        #pragma unroll kFixedMaxVecsPerThread
        for (int32_t vec = 0; vec < kFixedMaxVecsPerThread && (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth < D; ++vec) {
            const int32_t d = (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth;
            Vec4TAcc<grad_t> go(grad_output_ + d);
            grad_out[vec] = go;
        }

        for (int32_t l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            const auto offset_idx = l < L
                ? (static_cast<overflow_safe_int_t>(indices[indices_start + l]) * D_emb)
                : 0;
            const auto cache_idx =
                (placement == PlacementType::MANAGED_CACHING && l < L)
                    ? lxu_cache_locations[indices_start + l] : 0;
            FBGEMM_KERNEL_ERROR_CHECK(
                1, offset_idx >= 0 && offset_idx < weights_numel, offset_idx
            )
            FBGEMM_KERNEL_ERROR_CHECK(
                2, offset_idx + D_emb <= weights_numel, offset_idx
            )

            int32_t j = 0;
            for (; j < kWarpSize && l_start + j < L; ++j) {
                const auto offset_idx_j = shfl_sync(offset_idx, j);
                const auto cache_idx_j = shfl_sync(cache_idx, j);

                at::acc_type<cache_t, true> grad_indice_weight = 0.0;
                [[maybe_unused]] const auto weight_row =
                    WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j], D);

                #pragma unroll kFixedMaxVecsPerThread
                for (int32_t vec = 0;
                    vec < kFixedMaxVecsPerThread && (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth < D;
                    ++vec) {
                    const int32_t d = (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth;
                    if (
                      (
                          placement == PlacementType::MANAGED_CACHING
                          && (cache_idx_j != kCacheLocationMissing)
                      )
                    ) {
                        const cache_t* cache_weights =
                          &lxu_cache_weights[cache_idx_j][d];
                        Vec4T<cache_t> weight(cache_weights);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    } else {
                        const auto weight = weight_row.load(d);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    }
                }
                grad_indice_weight =
                    warpReduceAllSum<at::acc_type<cache_t, true>, kWarpSize, embDimMatch>(grad_indice_weight);
                if (threadIdx.x == 0) {
                    if (vec_start == 0) {
                        grad_indice_weights[indices_start + l_start + j] =
                            grad_indice_weight;
                    }
                    else {
                        grad_indice_weights[indices_start + l_start + j] +=
                            grad_indice_weight;
                    }
                }
            }
        }
    } // for vec_start
kernel_error_handler:
    FBGEMM_KERNEL_ERROR_THROW(
        1,
        offset_idx >= 0 && offset_idx < weights_numel,
        (offset_idx=%lld, weights_numel=%lld),
        error_value,
        weights_numel
    )
    FBGEMM_KERNEL_ERROR_THROW(
        2,
        offset_idx + D_emb <= weights_numel,
        (offset_idx=%lld, D_emb=%d, weights_numel=%lld),
        error_value,
        D_emb,
        weights_numel
    )
}

// TODO: optimization to use multiple warps per row.
template <
  typename emb_t,
  typename grad_t,
  typename cache_t,
  typename index_t,
  int32_t kFixedMaxVecsPerThread,
  bool embDimMatch
>
__global__ __launch_bounds__(kForwardMaxThreads) void
split_embedding_codegen_grad_indice_weights_vbe_kernel(
    // [\sum_t E_t x D_t]
    const pta::PackedTensorAccessor64<grad_t, 2, at::RestrictPtrTraits> grad_output,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> dev_weights,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> uvm_weights,
    pta::PackedTensorAccessor64<cache_t, 2, at::RestrictPtrTraits> lxu_cache_weights,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> weights_placements,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> weights_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> D_offsets,
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> indices, // [N = \sum_{b,t} L_{b,t} total indices, i.e. flattened [B][T][L]
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> offsets, // [B x T + 1]
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> lxu_cache_locations,
    pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> feature_requires_grad, // [T],
    pta::PackedTensorAccessor32<at::acc_type<cache_t, true>, 1, at::RestrictPtrTraits> grad_indice_weights,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> row_grad_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> b_t_map,
    const int32_t info_B_num_bits,
    const uint32_t info_B_mask
    ) {
    constexpr int32_t kVecWidth = 4;
    int error_code = 0;
    int64_t error_value;

    int32_t T = D_offsets.size(0) - 1;
    auto b_t = blockIdx.x * blockDim.y + threadIdx.y;
    if (b_t >= offsets.size(0) - 1) {
        return;
    }

    int32_t t;
    [[maybe_unused]] int32_t b;
    const auto info = reinterpret_cast<const uint32_t*>(&b_t_map[b_t])[0];
    reinterpret_cast<uint32_t*>(&t)[0] = info >> info_B_num_bits;
    reinterpret_cast<uint32_t*>(&b)[0] = info & info_B_mask;

    const auto weights_offset = weights_offsets[t];
    const auto D_start = D_offsets[t];
    const auto D_end = D_offsets[t + 1];
    const auto D = D_end - D_start;
    auto D_emb = D;
    if constexpr (std::is_same_v<emb_t, uint8_t>) {
      D_emb += kINT8QparamsBytes;
    }
    const auto indices_start = offsets[b_t];
    const auto indices_end = offsets[b_t + 1];
    const auto L = indices_end - indices_start;
    if (feature_requires_grad.size(0) > 0 && !feature_requires_grad[t]) {
        // If the table does not require gradient computation, we set the gradient to zero.
        for (auto l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            if (l < L) {
                grad_indice_weights[indices_start + l] = 0.0;
            }
        }
        return;
    }

    emb_t* __restrict__ weights;
    overflow_safe_int_t weights_numel;
    const auto placement = static_cast<PlacementType>(weights_placements[t]);
    if (placement == PlacementType::DEVICE) {
        weights = &dev_weights[weights_offset];
        weights_numel = dev_weights.size(0) - weights_offset;
    } else {
        weights = &uvm_weights[weights_offset];
        weights_numel = uvm_weights.size(0) - weights_offset;
    }
    const grad_t* grad_output_ = &grad_output[0][row_grad_offsets[b_t]];

    Vec4TAcc<cache_t> grad_out[kFixedMaxVecsPerThread]; 

        // Load gradients
        // TODO: Maybe using a combination of shared memory and registers is
        // better for performance
        #pragma unroll kFixedMaxVecsPerThread
        for (int32_t vec = 0; vec < kFixedMaxVecsPerThread && (kWarpSize * vec + threadIdx.x) * kVecWidth < D; ++vec) {
            const int32_t d = (kWarpSize * vec + threadIdx.x) * kVecWidth;
            Vec4TAcc<grad_t> go(grad_output_ + d);
            grad_out[vec] = go;
        }

        for (int32_t l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            const auto offset_idx = l < L
                ? (static_cast<overflow_safe_int_t>(indices[indices_start + l]) * D_emb)
                : 0;
            const auto cache_idx =
                (placement == PlacementType::MANAGED_CACHING && l < L)
                    ? lxu_cache_locations[indices_start + l] : 0;
            FBGEMM_KERNEL_ERROR_CHECK(
                1, offset_idx >= 0 && offset_idx < weights_numel, offset_idx
            )
            FBGEMM_KERNEL_ERROR_CHECK(
                2, offset_idx + D_emb <= weights_numel, offset_idx
            )

            int32_t j = 0;
            for (; j < kWarpSize && l_start + j < L; ++j) {
                const auto offset_idx_j = shfl_sync(offset_idx, j);
                const auto cache_idx_j = shfl_sync(cache_idx, j);

                at::acc_type<cache_t, true> grad_indice_weight = 0.0;
                [[maybe_unused]] const auto weight_row =
                    WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j], D);

                #pragma unroll kFixedMaxVecsPerThread
                for (int32_t vec = 0;
                    vec < kFixedMaxVecsPerThread && (kWarpSize * vec + threadIdx.x) * kVecWidth < D;
                    ++vec) {
                    const int32_t d = (kWarpSize * vec + threadIdx.x) * kVecWidth;
                    if (
                      (
                          placement == PlacementType::MANAGED_CACHING
                          && (cache_idx_j != kCacheLocationMissing)
                      )
                    ) {
                        const cache_t* cache_weights =
                          &lxu_cache_weights[cache_idx_j][d];
                        Vec4T<cache_t> weight(cache_weights);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    } else {
                        const auto weight = weight_row.load(d);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    }
                }
                grad_indice_weight =
                    warpReduceAllSum<at::acc_type<cache_t, true>, kWarpSize, embDimMatch>(grad_indice_weight);
                if (threadIdx.x == 0) {
                    grad_indice_weights[indices_start + l_start + j] =
                        grad_indice_weight;
                }
            }
        }
kernel_error_handler:
    FBGEMM_KERNEL_ERROR_THROW(
        1,
        offset_idx >= 0 && offset_idx < weights_numel,
        (offset_idx=%lld, weights_numel=%lld),
        error_value,
        weights_numel
    )
    FBGEMM_KERNEL_ERROR_THROW(
        2,
        offset_idx + D_emb <= weights_numel,
        (offset_idx=%lld, D_emb=%d, weights_numel=%lld),
        error_value,
        D_emb,
        weights_numel
    )
} 

Tensor split_embedding_codegen_grad_indice_weights_vbe_cuda(
    const Tensor& grad_output,
    const Tensor& dev_weights,
    const Tensor& uvm_weights,
    const Tensor& lxu_cache_weights,
    const Tensor& weights_placements,
    const Tensor& weights_offsets,
    const Tensor& D_offsets,
    const c10::SymInt max_D_,
    const Tensor& indices,
    const Tensor& offsets,
    const Tensor& lxu_cache_locations,
    const Tensor& feature_requires_grad,
    const Tensor& vbe_row_output_offsets,
    const Tensor& vbe_b_t_map,
    const int64_t info_B_num_bits, // int32_t
    const int64_t info_B_mask_int64 // uint32_t
) {
   const int64_t max_D = max_D_.guard_int(__FILE__, __LINE__);
   TENSORS_ON_SAME_CUDA_GPU_IF_NOT_OPTIONAL(
        dev_weights,
        uvm_weights,
        lxu_cache_weights,
        weights_placements,
        weights_offsets,
        D_offsets,
        indices,
        offsets,
        lxu_cache_locations,
        vbe_row_output_offsets,
        vbe_b_t_map,
        grad_output
    );

    if (feature_requires_grad.defined()) {
        TENSOR_ON_CUDA_GPU(feature_requires_grad);
    }

    auto aligned_grad_output = aligned_grad_output_tensor_for_cuda_backwards(grad_output);

    CUDA_DEVICE_GUARD(dev_weights);
    #ifdef USE_ROCM
        if (!rocm::is_supported_cdna()) {
            TORCH_WARN_ONCE("Running on non-CDNA architecture. Performance may be suboptimal.");
        }
        else {
            // Ensure we're running on a supported CDNA architecture (including MI350)
            TORCH_WARN_ONCE("Running on CDNA architecture");
        }
    #endif
    
    const auto T = D_offsets.size(0) - 1;
    TORCH_CHECK_GT(T, 0);
    // offsets = [B x T  + 1]
    const auto total_B = offsets.size(0) - 1;
    TORCH_CHECK_GE(total_B, 0);
    TORCH_CHECK_LE(max_D, 2048);
    auto grad_indice_weights = empty_like(indices, indices.options().dtype(
          at::toAccumulateType(aligned_grad_output.scalar_type(), true)));

    if (total_B == 0) {
      return grad_indice_weights;
    }

    const auto feature_requires_grad_ = feature_requires_grad.defined()
        ? feature_requires_grad
        : at::empty({0}, indices.options().dtype(at::kInt));
    // Cast info_B_mask from int64_t to uint32_t
    const uint32_t info_B_mask = info_B_mask_int64;

    AT_DISPATCH_INDEX_TYPES(indices.scalar_type(), "split_embedding_codegen_grad_indice_weights_vbe_kernel_1", [&] {
    DISPATCH_EMB_GRAD_CACHE_TYPES(
        dev_weights.scalar_type(),
        aligned_grad_output.scalar_type(),
        lxu_cache_weights.scalar_type(),
        "split_embedding_codegen_grad_indice_weights_vbe_kernel_2",
        [&] {
            const auto& grad_output_reshaped = aligned_grad_output.reshape({1, -1});
            DISPATCH_NON_VEC_BLOCKING_KERNEL(max_D, [&] {
                auto kernel_name_ = split_embedding_codegen_grad_indice_weights_vbe_kernel<
                        emb_t,
                        grad_t,
                        cache_t,
                        index_t,
                        kFixedMaxVecsPerThread,
                        /*embDimMatch=*/ false>;
#ifdef USE_ROCM
#endif          
                FBGEMM_LAUNCH_KERNEL(
                    kernel_name_,
                    div_round_up(total_B, kForwardMaxThreads / kWarpSize),
                    dim3(kWarpSize, kForwardMaxThreads / kWarpSize),
                    0,
                    at::hip::getCurrentHIPStreamMasqueradingAsCUDA(),
                    PTA_B(grad_output_reshaped, grad_t, 2, 64),
                    PTA_B(dev_weights, emb_t, 1, 64),
                    PTA_B(uvm_weights, emb_t, 1, 64),
                    PTA_B(lxu_cache_weights, cache_t, 2, 64),
                    PTA_B(weights_placements, int32_t, 1, 32),
                    PTA_B(weights_offsets, int64_t, 1, 32),
                    PTA_B(D_offsets, int32_t, 1, 32),
                    PTA_B(indices, index_t, 1, 32),
                    PTA_B(offsets, index_t, 1, 32),
                    PTA_B(lxu_cache_locations, int32_t, 1, 32),
                    PTA_B(feature_requires_grad_, int32_t, 1, 32),
                    PTA_ACC_B(grad_indice_weights, grad_t, 1, 32),
                    PTA_B(vbe_row_output_offsets, int64_t, 1, 32),
                    PTA_B(vbe_b_t_map, int32_t, 1, 32),
                    info_B_num_bits,
                    info_B_mask
                );
                return;
            });
            DISPATCH_VEC_BLOCKING_KERNEL(max_D, [&] {
                auto kernel_name_ = split_embedding_codegen_grad_indice_weights_vbe_vec_blocking_kernel<
                        emb_t,
                        grad_t,
                        cache_t,
                        index_t,
                        kFixedMaxVecsPerThread,
                        /*embDimMatch=*/ false>;
#ifdef USE_ROCM
#endif          
                FBGEMM_LAUNCH_KERNEL(
                    kernel_name_,
                    div_round_up(total_B, kForwardMaxThreads / kWarpSize),
                    dim3(kWarpSize, kForwardMaxThreads / kWarpSize),
                    0,
                    at::hip::getCurrentHIPStreamMasqueradingAsCUDA(),
                    PTA_B(grad_output_reshaped, grad_t, 2, 64),
                    PTA_B(dev_weights, emb_t, 1, 64),
                    PTA_B(uvm_weights, emb_t, 1, 64),
                    PTA_B(lxu_cache_weights, cache_t, 2, 64),
                    PTA_B(weights_placements, int32_t, 1, 32),
                    PTA_B(weights_offsets, int64_t, 1, 32),
                    PTA_B(D_offsets, int32_t, 1, 32),
                    PTA_B(indices, index_t, 1, 32),
                    PTA_B(offsets, index_t, 1, 32),
                    PTA_B(lxu_cache_locations, int32_t, 1, 32),
                    PTA_B(feature_requires_grad_, int32_t, 1, 32),
                    PTA_ACC_B(grad_indice_weights, grad_t, 1, 32),
                    PTA_B(vbe_row_output_offsets, int64_t, 1, 32),
                    PTA_B(vbe_b_t_map, int32_t, 1, 32),
                    info_B_num_bits,
                    info_B_mask
                );
                return;
            }); 
        });
    });

  return grad_indice_weights;
}


Tensor split_embedding_codegen_grad_indice_weights_vbe_meta(
    const Tensor& grad_output,
    const Tensor& dev_weights,
    const Tensor& uvm_weights,
    const Tensor& lxu_cache_weights,
    const Tensor& weights_placements,
    const Tensor& weights_offsets,
    const Tensor& D_offsets,
    const c10::SymInt max_D,
    const Tensor& indices,
    const Tensor& offsets,
    const Tensor& lxu_cache_locations,
    const Tensor& feature_requires_grad,
    const Tensor& vbe_row_output_offsets,
    const Tensor& vbe_b_t_map,
    const int64_t info_B_num_bits, // int32_t
    const int64_t info_B_mask_int64 // uint32_t
) {

    const auto T = D_offsets.sym_size(0) - 1;
    TORCH_CHECK_GT(T, 0);
    // offsets = [B x T  + 1]
    const auto total_B = offsets.sym_size(0) - 1;
    TORCH_CHECK_GE(total_B, 0);
    TORCH_CHECK_LE(max_D, 2048);

    auto grad_indice_weights = empty_like(indices, indices.options().dtype(
          at::toAccumulateType(grad_output.scalar_type(), true)));

    return grad_indice_weights;
}

////////////////////////////////////////////////////////////////////////////////
// Op registrations
////////////////////////////////////////////////////////////////////////////////
TORCH_LIBRARY_FRAGMENT(fbgemm, m) {
    m.def("split_embedding_codegen_grad_indice_weights_vbe_cuda("
          "    Tensor grad_output, "
          "    Tensor dev_weights, "
          "    Tensor uvm_weights, "
          "    Tensor lxu_cache_weights, "
          "    Tensor weights_placements, "
          "    Tensor weights_offsets, "
          "    Tensor D_offsets, "
          "    SymInt max_D, "
          "    Tensor indices, "
          "    Tensor offsets, "
          "    Tensor lxu_cache_locations, "
          "    Tensor feature_requires_grad, "
          "    Tensor vbe_row_output_offsets, "
          "    Tensor vbe_b_t_map, "
          "    int info_B_num_bits, "
          "    int info_B_mask_int64"
          ") -> Tensor");
    DISPATCH_TO_CUDA(
        "split_embedding_codegen_grad_indice_weights_vbe_cuda",
        split_embedding_codegen_grad_indice_weights_vbe_cuda
    );
    m.impl("split_embedding_codegen_grad_indice_weights_vbe_cuda",
        torch::dispatch(c10::DispatchKey::Meta,
          TORCH_FN(split_embedding_codegen_grad_indice_weights_vbe_meta)));
}

// TODO: optimization to use multiple warps per row.
template <
  typename emb_t,
  typename grad_t,
  typename cache_t,
  typename index_t,
  int32_t kFixedMaxVecsPerThread,
  bool embDimMatch
>
__global__ __launch_bounds__(kForwardMaxThreads) void
split_embedding_codegen_grad_indice_weights_vec_blocking_kernel(
    // [\sum_t E_t x D_t]
    const pta::PackedTensorAccessor64<grad_t, 2, at::RestrictPtrTraits> grad_output,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> dev_weights,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> uvm_weights,
    pta::PackedTensorAccessor64<cache_t, 2, at::RestrictPtrTraits> lxu_cache_weights,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> weights_placements,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> weights_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> D_offsets,
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> indices, // [N = \sum_{b,t} L_{b,t} total indices, i.e. flattened [B][T][L]
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> offsets, // [B x T + 1]
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> lxu_cache_locations,
    pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> feature_requires_grad, // [T],
    pta::PackedTensorAccessor32<at::acc_type<cache_t, true>, 1, at::RestrictPtrTraits> grad_indice_weights,
    FixedDivisor fd_B
    ) {
    constexpr int32_t kVecWidth = 4;
    int error_code = 0;
    int64_t error_value;

    int32_t T = D_offsets.size(0) - 1;
    auto b_t = blockIdx.x * blockDim.y + threadIdx.y;
    if (b_t >= offsets.size(0) - 1) {
        return;
    }

    int32_t t;
    [[maybe_unused]] int32_t b;
    fd_B.DivMod(b_t, &t, &b);

    const auto weights_offset = weights_offsets[t];
    const auto D_start = D_offsets[t];
    const auto D_end = D_offsets[t + 1];
    const auto D = D_end - D_start;
    auto D_emb = D;
    if constexpr (std::is_same_v<emb_t, uint8_t>) {
      D_emb += kINT8QparamsBytes;
    }
    const auto indices_start = offsets[b_t];
    const auto indices_end = offsets[b_t + 1];
    const auto L = indices_end - indices_start;
    if (feature_requires_grad.size(0) > 0 && !feature_requires_grad[t]) {
        // If the table does not require gradient computation, we set the gradient to zero.
        for (auto l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            if (l < L) {
                grad_indice_weights[indices_start + l] = 0.0;
            }
        }
        return;
    }

    emb_t* __restrict__ weights;
    overflow_safe_int_t weights_numel;
    const auto placement = static_cast<PlacementType>(weights_placements[t]);
    if (placement == PlacementType::DEVICE) {
        weights = &dev_weights[weights_offset];
        weights_numel = dev_weights.size(0) - weights_offset;
    } else {
        weights = &uvm_weights[weights_offset];
        weights_numel = uvm_weights.size(0) - weights_offset;
    }
    const grad_t* grad_output_ = &grad_output[b][D_start];

    Vec4TAcc<cache_t> grad_out[kFixedMaxVecsPerThread];
    const int32_t num_vecs = div_round_up(D, kWarpSize * kVecWidth);
    for (int32_t vec_start = 0;
         vec_start < num_vecs;
         vec_start += kFixedMaxVecsPerThread) { 

        // Load gradients
        // TODO: Maybe using a combination of shared memory and registers is
        // better for performance
        #pragma unroll kFixedMaxVecsPerThread
        for (int32_t vec = 0; vec < kFixedMaxVecsPerThread && (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth < D; ++vec) {
            const int32_t d = (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth;
            Vec4TAcc<grad_t> go(grad_output_ + d);
            grad_out[vec] = go;
        }

        for (int32_t l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            const auto offset_idx = l < L
                ? (static_cast<overflow_safe_int_t>(indices[indices_start + l]) * D_emb)
                : 0;
            const auto cache_idx =
                (placement == PlacementType::MANAGED_CACHING && l < L)
                    ? lxu_cache_locations[indices_start + l] : 0;
            FBGEMM_KERNEL_ERROR_CHECK(
                1, offset_idx >= 0 && offset_idx < weights_numel, offset_idx
            )
            FBGEMM_KERNEL_ERROR_CHECK(
                2, offset_idx + D_emb <= weights_numel, offset_idx
            )

            int32_t j = 0;
            for (; j < kWarpSize && l_start + j < L; ++j) {
                const auto offset_idx_j = shfl_sync(offset_idx, j);
                const auto cache_idx_j = shfl_sync(cache_idx, j);

                at::acc_type<cache_t, true> grad_indice_weight = 0.0;
                [[maybe_unused]] const auto weight_row =
                    WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j], D);

                #pragma unroll kFixedMaxVecsPerThread
                for (int32_t vec = 0;
                    vec < kFixedMaxVecsPerThread && (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth < D;
                    ++vec) {
                    const int32_t d = (kWarpSize * (vec + vec_start) + threadIdx.x) * kVecWidth;
                    if (
                      (
                          placement == PlacementType::MANAGED_CACHING
                          && (cache_idx_j != kCacheLocationMissing)
                      )
                    ) {
                        const cache_t* cache_weights =
                          &lxu_cache_weights[cache_idx_j][d];
                        Vec4T<cache_t> weight(cache_weights);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    } else {
                        const auto weight = weight_row.load(d);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    }
                }
                grad_indice_weight =
                    warpReduceAllSum<at::acc_type<cache_t, true>, kWarpSize, embDimMatch>(grad_indice_weight);
                if (threadIdx.x == 0) {
                    if (vec_start == 0) {
                        grad_indice_weights[indices_start + l_start + j] =
                            grad_indice_weight;
                    }
                    else {
                        grad_indice_weights[indices_start + l_start + j] +=
                            grad_indice_weight;
                    }
                }
            }
        }
    } // for vec_start
kernel_error_handler:
    FBGEMM_KERNEL_ERROR_THROW(
        1,
        offset_idx >= 0 && offset_idx < weights_numel,
        (offset_idx=%lld, weights_numel=%lld),
        error_value,
        weights_numel
    )
    FBGEMM_KERNEL_ERROR_THROW(
        2,
        offset_idx + D_emb <= weights_numel,
        (offset_idx=%lld, D_emb=%d, weights_numel=%lld),
        error_value,
        D_emb,
        weights_numel
    )
}

// TODO: optimization to use multiple warps per row.
template <
  typename emb_t,
  typename grad_t,
  typename cache_t,
  typename index_t,
  int32_t kFixedMaxVecsPerThread,
  bool embDimMatch
>
__global__ __launch_bounds__(kForwardMaxThreads) void
split_embedding_codegen_grad_indice_weights_kernel(
    // [\sum_t E_t x D_t]
    const pta::PackedTensorAccessor64<grad_t, 2, at::RestrictPtrTraits> grad_output,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> dev_weights,
    pta::PackedTensorAccessor64<emb_t, 1, at::RestrictPtrTraits> uvm_weights,
    pta::PackedTensorAccessor64<cache_t, 2, at::RestrictPtrTraits> lxu_cache_weights,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> weights_placements,
    const pta::PackedTensorAccessor32<int64_t, 1, at::RestrictPtrTraits> weights_offsets,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> D_offsets,
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> indices, // [N = \sum_{b,t} L_{b,t} total indices, i.e. flattened [B][T][L]
    const pta::PackedTensorAccessor32<index_t, 1, at::RestrictPtrTraits> offsets, // [B x T + 1]
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> lxu_cache_locations,
    pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits> feature_requires_grad, // [T],
    pta::PackedTensorAccessor32<at::acc_type<cache_t, true>, 1, at::RestrictPtrTraits> grad_indice_weights,
    FixedDivisor fd_B
    ) {
    constexpr int32_t kVecWidth = 4;
    int error_code = 0;
    int64_t error_value;

    int32_t T = D_offsets.size(0) - 1;
    auto b_t = blockIdx.x * blockDim.y + threadIdx.y;
    if (b_t >= offsets.size(0) - 1) {
        return;
    }

    int32_t t;
    [[maybe_unused]] int32_t b;
    fd_B.DivMod(b_t, &t, &b);

    const auto weights_offset = weights_offsets[t];
    const auto D_start = D_offsets[t];
    const auto D_end = D_offsets[t + 1];
    const auto D = D_end - D_start;
    auto D_emb = D;
    if constexpr (std::is_same_v<emb_t, uint8_t>) {
      D_emb += kINT8QparamsBytes;
    }
    const auto indices_start = offsets[b_t];
    const auto indices_end = offsets[b_t + 1];
    const auto L = indices_end - indices_start;
    if (feature_requires_grad.size(0) > 0 && !feature_requires_grad[t]) {
        // If the table does not require gradient computation, we set the gradient to zero.
        for (auto l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            if (l < L) {
                grad_indice_weights[indices_start + l] = 0.0;
            }
        }
        return;
    }

    emb_t* __restrict__ weights;
    overflow_safe_int_t weights_numel;
    const auto placement = static_cast<PlacementType>(weights_placements[t]);
    if (placement == PlacementType::DEVICE) {
        weights = &dev_weights[weights_offset];
        weights_numel = dev_weights.size(0) - weights_offset;
    } else {
        weights = &uvm_weights[weights_offset];
        weights_numel = uvm_weights.size(0) - weights_offset;
    }
    const grad_t* grad_output_ = &grad_output[b][D_start];

    Vec4TAcc<cache_t> grad_out[kFixedMaxVecsPerThread]; 

        // Load gradients
        // TODO: Maybe using a combination of shared memory and registers is
        // better for performance
        #pragma unroll kFixedMaxVecsPerThread
        for (int32_t vec = 0; vec < kFixedMaxVecsPerThread && (kWarpSize * vec + threadIdx.x) * kVecWidth < D; ++vec) {
            const int32_t d = (kWarpSize * vec + threadIdx.x) * kVecWidth;
            Vec4TAcc<grad_t> go(grad_output_ + d);
            grad_out[vec] = go;
        }

        for (int32_t l_start = 0; l_start < L; l_start += kWarpSize) {
            auto l = l_start + threadIdx.x;
            const auto offset_idx = l < L
                ? (static_cast<overflow_safe_int_t>(indices[indices_start + l]) * D_emb)
                : 0;
            const auto cache_idx =
                (placement == PlacementType::MANAGED_CACHING && l < L)
                    ? lxu_cache_locations[indices_start + l] : 0;
            FBGEMM_KERNEL_ERROR_CHECK(
                1, offset_idx >= 0 && offset_idx < weights_numel, offset_idx
            )
            FBGEMM_KERNEL_ERROR_CHECK(
                2, offset_idx + D_emb <= weights_numel, offset_idx
            )

            int32_t j = 0;
            // Currently for split_embedding_codegen_grad_indice_weights_kernel only
            for (; j < kWarpSize && l_start + j + 3 < L; j += 4) {
                const auto offset_idx_j0 = shfl_sync(offset_idx, j);
                const auto offset_idx_j1 = shfl_sync(offset_idx, j+1);
                const auto offset_idx_j2 = shfl_sync(offset_idx, j+2);
                const auto offset_idx_j3 = shfl_sync(offset_idx, j+3);

                const auto cache_idx_j0 = shfl_sync(cache_idx, j);
                const auto cache_idx_j1 = shfl_sync(cache_idx, j+1);
                const auto cache_idx_j2 = shfl_sync(cache_idx, j+2);
                const auto cache_idx_j3 = shfl_sync(cache_idx, j+3);

                at::acc_type<cache_t, true> grad_indice_weight0 = 0.0;
                at::acc_type<cache_t, true> grad_indice_weight1 = 0.0;
                at::acc_type<cache_t, true> grad_indice_weight2 = 0.0;
                at::acc_type<cache_t, true> grad_indice_weight3 = 0.0;

                [[maybe_unused]] const auto weight_row0 = WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j0], D);
                [[maybe_unused]] const auto weight_row1 = WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j1], D);
                [[maybe_unused]] const auto weight_row2 = WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j2], D);
                [[maybe_unused]] const auto weight_row3 = WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j3], D);

                #pragma unroll kFixedMaxVecsPerThread
                for (int32_t vec = 0; vec < kFixedMaxVecsPerThread && (kWarpSize * vec + threadIdx.x) * kVecWidth < D; ++vec) {
                    const int32_t d = (kWarpSize * vec + threadIdx.x) * kVecWidth;

                    Vec4T<at::acc_type<cache_t, true>> weight0, weight1, weight2, weight3;
                    if (placement == PlacementType::MANAGED_CACHING) {
                        weight0 = (cache_idx_j0 != kCacheLocationMissing) ?
                        Vec4T<at::acc_type<cache_t, true>>(&lxu_cache_weights[cache_idx_j0][d]) :
                        weight_row0.load(d);

                        weight1 = (cache_idx_j1 != kCacheLocationMissing) ?
                        Vec4T<at::acc_type<cache_t, true>>(&lxu_cache_weights[cache_idx_j1][d]) :
                        weight_row1.load(d);

                        weight2 = (cache_idx_j2 != kCacheLocationMissing) ?
                        Vec4T<at::acc_type<cache_t, true>>(&lxu_cache_weights[cache_idx_j2][d]) :
                        weight_row2.load(d);

                        weight3 = (cache_idx_j3 != kCacheLocationMissing) ?
                        Vec4T<at::acc_type<cache_t, true>>(&lxu_cache_weights[cache_idx_j3][d]) :
                        weight_row3.load(d);
                    } else {
                        weight0 = weight_row0.load(d);
                        weight1 = weight_row1.load(d);
                        weight2 = weight_row2.load(d);
                        weight3 = weight_row3.load(d);
                    }

                    grad_indice_weight0 += weight0.acc.x * grad_out[vec].acc.x + weight0.acc.y * grad_out[vec].acc.y +
                            weight0.acc.z * grad_out[vec].acc.z + weight0.acc.w * grad_out[vec].acc.w;
                    grad_indice_weight1 += weight1.acc.x * grad_out[vec].acc.x + weight1.acc.y * grad_out[vec].acc.y +
                        weight1.acc.z * grad_out[vec].acc.z + weight1.acc.w * grad_out[vec].acc.w;
                    grad_indice_weight2 += weight2.acc.x * grad_out[vec].acc.x + weight2.acc.y * grad_out[vec].acc.y +
                        weight2.acc.z * grad_out[vec].acc.z + weight2.acc.w * grad_out[vec].acc.w;
                    grad_indice_weight3 += weight3.acc.x * grad_out[vec].acc.x + weight3.acc.y * grad_out[vec].acc.y +
                        weight3.acc.z * grad_out[vec].acc.z + weight3.acc.w * grad_out[vec].acc.w;
                }
                
                grad_indice_weight0 = warpReduceAllSum<at::acc_type<cache_t, true>>(grad_indice_weight0);
                grad_indice_weight1 = warpReduceAllSum<at::acc_type<cache_t, true>>(grad_indice_weight1);
                grad_indice_weight2 = warpReduceAllSum<at::acc_type<cache_t, true>>(grad_indice_weight2);
                grad_indice_weight3 = warpReduceAllSum<at::acc_type<cache_t, true>>(grad_indice_weight3);

                if (threadIdx.x == 0) {
                    grad_indice_weights[indices_start + l_start + j] = grad_indice_weight0;
                    grad_indice_weights[indices_start + l_start + j+1] = grad_indice_weight1;
                    grad_indice_weights[indices_start + l_start + j+2] = grad_indice_weight2;
                    grad_indice_weights[indices_start + l_start + j+3] = grad_indice_weight3;
                }
            }
            for (; j < kWarpSize && l_start + j < L; ++j) {
                const auto offset_idx_j = shfl_sync(offset_idx, j);
                const auto cache_idx_j = shfl_sync(cache_idx, j);

                at::acc_type<cache_t, true> grad_indice_weight = 0.0;
                [[maybe_unused]] const auto weight_row =
                    WeightRowAccessor<emb_t, at::acc_type<cache_t, true>>(&weights[offset_idx_j], D);

                #pragma unroll kFixedMaxVecsPerThread
                for (int32_t vec = 0;
                    vec < kFixedMaxVecsPerThread && (kWarpSize * vec + threadIdx.x) * kVecWidth < D;
                    ++vec) {
                    const int32_t d = (kWarpSize * vec + threadIdx.x) * kVecWidth;
                    if (
                      (
                          placement == PlacementType::MANAGED_CACHING
                          && (cache_idx_j != kCacheLocationMissing)
                      )
                    ) {
                        const cache_t* cache_weights =
                          &lxu_cache_weights[cache_idx_j][d];
                        Vec4T<cache_t> weight(cache_weights);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    } else {
                        const auto weight = weight_row.load(d);
                        grad_indice_weight += weight.acc.x * grad_out[vec].acc.x +
                            weight.acc.y * grad_out[vec].acc.y +
                            weight.acc.z * grad_out[vec].acc.z +
                            weight.acc.w * grad_out[vec].acc.w;
                    }
                }
                grad_indice_weight =
                    warpReduceAllSum<at::acc_type<cache_t, true>, kWarpSize, embDimMatch>(grad_indice_weight);
                if (threadIdx.x == 0) {
                    grad_indice_weights[indices_start + l_start + j] =
                        grad_indice_weight;
                }
            }
        }
kernel_error_handler:
    FBGEMM_KERNEL_ERROR_THROW(
        1,
        offset_idx >= 0 && offset_idx < weights_numel,
        (offset_idx=%lld, weights_numel=%lld),
        error_value,
        weights_numel
    )
    FBGEMM_KERNEL_ERROR_THROW(
        2,
        offset_idx + D_emb <= weights_numel,
        (offset_idx=%lld, D_emb=%d, weights_numel=%lld),
        error_value,
        D_emb,
        weights_numel
    )
} 

Tensor split_embedding_codegen_grad_indice_weights_cuda(
    const Tensor& grad_output,
    const Tensor& dev_weights,
    const Tensor& uvm_weights,
    const Tensor& lxu_cache_weights,
    const Tensor& weights_placements,
    const Tensor& weights_offsets,
    const Tensor& D_offsets,
    const c10::SymInt max_D_,
    const Tensor& indices,
    const Tensor& offsets,
    const Tensor& lxu_cache_locations,
    const Tensor& feature_requires_grad
) {
   const int64_t max_D = max_D_.guard_int(__FILE__, __LINE__);
   TENSORS_ON_SAME_CUDA_GPU_IF_NOT_OPTIONAL(
        dev_weights,
        uvm_weights,
        lxu_cache_weights,
        weights_placements,
        weights_offsets,
        D_offsets,
        indices,
        offsets,
        lxu_cache_locations,
        grad_output
    );

    if (feature_requires_grad.defined()) {
        TENSOR_ON_CUDA_GPU(feature_requires_grad);
    }

    auto aligned_grad_output = aligned_grad_output_tensor_for_cuda_backwards(grad_output);

    CUDA_DEVICE_GUARD(dev_weights);
    #ifdef USE_ROCM
        if (!rocm::is_supported_cdna()) {
            TORCH_WARN_ONCE("Running on non-CDNA architecture. Performance may be suboptimal.");
        }
        else {
            // Ensure we're running on a supported CDNA architecture (including MI350)
            TORCH_WARN_ONCE("Running on CDNA architecture");
        }
    #endif
    
    const auto T = D_offsets.size(0) - 1;
    TORCH_CHECK_GT(T, 0);
    // offsets = [B x T  + 1]
    const auto total_B = offsets.size(0) - 1;
    TORCH_CHECK_GE(total_B, 0);
    TORCH_CHECK_LE(max_D, 2048);
    auto grad_indice_weights = empty_like(indices, indices.options().dtype(
          at::toAccumulateType(aligned_grad_output.scalar_type(), true)));

    if (total_B == 0) {
      return grad_indice_weights;
    }

    const auto feature_requires_grad_ = feature_requires_grad.defined()
        ? feature_requires_grad
        : at::empty({0}, indices.options().dtype(at::kInt));

    AT_DISPATCH_INDEX_TYPES(indices.scalar_type(), "split_embedding_codegen_grad_indice_weights_kernel_1", [&] {
    DISPATCH_EMB_GRAD_CACHE_TYPES(
        dev_weights.scalar_type(),
        aligned_grad_output.scalar_type(),
        lxu_cache_weights.scalar_type(),
        "split_embedding_codegen_grad_indice_weights_kernel_2",
        [&] {
            const auto& grad_output_reshaped = aligned_grad_output;
            DISPATCH_NON_VEC_BLOCKING_KERNEL(max_D, [&] {
                auto kernel_name_ = split_embedding_codegen_grad_indice_weights_kernel<
                        emb_t,
                        grad_t,
                        cache_t,
                        index_t,
                        kFixedMaxVecsPerThread,
                        /*embDimMatch=*/ false>;
#ifdef USE_ROCM
#endif          
                FBGEMM_LAUNCH_KERNEL(
                    kernel_name_,
                    div_round_up(total_B, kForwardMaxThreads / kWarpSize),
                    dim3(kWarpSize, kForwardMaxThreads / kWarpSize),
                    0,
                    at::hip::getCurrentHIPStreamMasqueradingAsCUDA(),
                    PTA_B(grad_output_reshaped, grad_t, 2, 64),
                    PTA_B(dev_weights, emb_t, 1, 64),
                    PTA_B(uvm_weights, emb_t, 1, 64),
                    PTA_B(lxu_cache_weights, cache_t, 2, 64),
                    PTA_B(weights_placements, int32_t, 1, 32),
                    PTA_B(weights_offsets, int64_t, 1, 32),
                    PTA_B(D_offsets, int32_t, 1, 32),
                    PTA_B(indices, index_t, 1, 32),
                    PTA_B(offsets, index_t, 1, 32),
                    PTA_B(lxu_cache_locations, int32_t, 1, 32),
                    PTA_B(feature_requires_grad_, int32_t, 1, 32),
                    PTA_ACC_B(grad_indice_weights, grad_t, 1, 32),
                    FixedDivisor(total_B / T)
                );
                return;
            });
            DISPATCH_VEC_BLOCKING_KERNEL(max_D, [&] {
                auto kernel_name_ = split_embedding_codegen_grad_indice_weights_vec_blocking_kernel<
                        emb_t,
                        grad_t,
                        cache_t,
                        index_t,
                        kFixedMaxVecsPerThread,
                        /*embDimMatch=*/ false>;
#ifdef USE_ROCM
#endif          
                FBGEMM_LAUNCH_KERNEL(
                    kernel_name_,
                    div_round_up(total_B, kForwardMaxThreads / kWarpSize),
                    dim3(kWarpSize, kForwardMaxThreads / kWarpSize),
                    0,
                    at::hip::getCurrentHIPStreamMasqueradingAsCUDA(),
                    PTA_B(grad_output_reshaped, grad_t, 2, 64),
                    PTA_B(dev_weights, emb_t, 1, 64),
                    PTA_B(uvm_weights, emb_t, 1, 64),
                    PTA_B(lxu_cache_weights, cache_t, 2, 64),
                    PTA_B(weights_placements, int32_t, 1, 32),
                    PTA_B(weights_offsets, int64_t, 1, 32),
                    PTA_B(D_offsets, int32_t, 1, 32),
                    PTA_B(indices, index_t, 1, 32),
                    PTA_B(offsets, index_t, 1, 32),
                    PTA_B(lxu_cache_locations, int32_t, 1, 32),
                    PTA_B(feature_requires_grad_, int32_t, 1, 32),
                    PTA_ACC_B(grad_indice_weights, grad_t, 1, 32),
                    FixedDivisor(total_B / T)
                );
                return;
            }); 
        });
    });

  return grad_indice_weights;
}


Tensor split_embedding_codegen_grad_indice_weights_meta(
    const Tensor& grad_output,
    const Tensor& dev_weights,
    const Tensor& uvm_weights,
    const Tensor& lxu_cache_weights,
    const Tensor& weights_placements,
    const Tensor& weights_offsets,
    const Tensor& D_offsets,
    const c10::SymInt max_D,
    const Tensor& indices,
    const Tensor& offsets,
    const Tensor& lxu_cache_locations,
    const Tensor& feature_requires_grad
) {

    const auto T = D_offsets.sym_size(0) - 1;
    TORCH_CHECK_GT(T, 0);
    // offsets = [B x T  + 1]
    const auto total_B = offsets.sym_size(0) - 1;
    TORCH_CHECK_GE(total_B, 0);
    TORCH_CHECK_LE(max_D, 2048);

    auto grad_indice_weights = empty_like(indices, indices.options().dtype(
          at::toAccumulateType(grad_output.scalar_type(), true)));

    return grad_indice_weights;
}

////////////////////////////////////////////////////////////////////////////////
// Op registrations
////////////////////////////////////////////////////////////////////////////////
TORCH_LIBRARY_FRAGMENT(fbgemm, m) {
    m.def("split_embedding_codegen_grad_indice_weights_cuda("
          "    Tensor grad_output, "
          "    Tensor dev_weights, "
          "    Tensor uvm_weights, "
          "    Tensor lxu_cache_weights, "
          "    Tensor weights_placements, "
          "    Tensor weights_offsets, "
          "    Tensor D_offsets, "
          "    SymInt max_D, "
          "    Tensor indices, "
          "    Tensor offsets, "
          "    Tensor lxu_cache_locations, "
          "    Tensor feature_requires_grad"
          ") -> Tensor");
    DISPATCH_TO_CUDA(
        "split_embedding_codegen_grad_indice_weights_cuda",
        split_embedding_codegen_grad_indice_weights_cuda
    );
    m.impl("split_embedding_codegen_grad_indice_weights_cuda",
        torch::dispatch(c10::DispatchKey::Meta,
          TORCH_FN(split_embedding_codegen_grad_indice_weights_meta)));
}
  // clang-format on