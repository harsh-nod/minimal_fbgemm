// !!! This is a file automatically generated by hipify!!!
////////////////////////////////////////////////////////////////////////////////
// GENERATED FILE INFO
//
// Template Source: training/backward/embedding_backward_split_device_kernel_template.cuh
////////////////////////////////////////////////////////////////////////////////



/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off

#include "fbgemm_gpu/embedding_backward_template_helpers_hip.cuh"
#include "fbgemm_gpu/utils/tensor_accessor_builder_hip.h"
#include "fbgemm_gpu/split_embeddings_utils_hip.cuh"

using namespace fbgemm_gpu;

template <
    typename grad_t,
    typename cache_t,
    int32_t kFixedMaxVecsPerThread,
    int32_t kThreadGroupSize = kWarpSize,
    int32_t VEC_WIDTH,
    bool kUseVecBlocking
>
DEVICE_INLINE void compute_grad_sum_unweighted(
    Vec4TAcc<cache_t>* grad_sum,
    Vec4TAcc<cache_t>* smem_grad_sum,
    const pta::PackedTensorAccessor64<grad_t, 2, at::RestrictPtrTraits>& grad_output,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits>& D_offsets,
    const int32_t D,
    const int32_t T,
    const pta::PackedTensorAccessor32<int32_t, 1, at::RestrictPtrTraits>& sorted_infos,
    const int32_t info_B_num_bits,
    const uint32_t info_B_mask,
    const int32_t segment_start,
    const int32_t sl_start,
    const int32_t sl_end,
    const unsigned int shfl_sync_mask,
    const int32_t num_vecs
) {
    // Copy value to vecs to make num_vecs known at compile time when
    // kUseVecBlocking == false
    const int32_t vecs = kUseVecBlocking ? num_vecs : kFixedMaxVecsPerThread;
    for (int32_t vec_start = 0;
         vec_start < vecs;
         vec_start += kFixedMaxVecsPerThread) {

        // Reset grad_sum vectors
        #pragma unroll kFixedMaxVecsPerThread
        for (int32_t vec = 0; vec < kFixedMaxVecsPerThread; vec++) {
            grad_sum[vec].acc.x = 0;
            grad_sum[vec].acc.y = 0;
            grad_sum[vec].acc.z = 0;
            grad_sum[vec].acc.w = 0;
        }

        for (int32_t sl = sl_start; sl < sl_end; sl += kThreadGroupSize) {
            auto sl_j = sl + threadIdx.x;
            const auto b_t = sl_j < sl_end
                ? reinterpret_cast<const uint32_t*>(
                    &sorted_infos[0])[segment_start + sl_j]
                : 0;
            const auto b = b_t & info_B_mask;
            const auto t = b_t >> info_B_num_bits; // if vbe
            int32_t D_start = sl_j < sl_end ? D_offsets[t] : 0; // if vbe // if not nobag
            for (int32_t j = 0; j < kThreadGroupSize && sl + j < sl_end; ++j) {
                int32_t b_j = SHFL_SYNC(b, j);
                int32_t D_start_j = SHFL_SYNC(D_start, j);

                #pragma unroll kFixedMaxVecsPerThread
                for (int32_t vec = 0; vec < kFixedMaxVecsPerThread && (((vec + vec_start) * kThreadGroupSize + threadIdx.x) * VEC_WIDTH) < D; ++vec) {
                    const int32_t d = (((vec + vec_start) * kThreadGroupSize + threadIdx.x) * VEC_WIDTH);
                    Vec4TAcc<grad_t> grad_out_vec(
                        &grad_output[b_j][0] + D_start_j + d // if nobag
                    );
                    grad_sum[vec].add_(grad_out_vec);
                }
            }
        }

        if (smem_grad_sum) {
            // Store grad_sum in smem_grad_sum
            #pragma unroll kFixedMaxVecsPerThread
            for (int32_t vec = 0;
                 (vec < kFixedMaxVecsPerThread) && ((vec + vec_start) * kThreadGroupSize + threadIdx.x) * VEC_WIDTH < D;
                 ++vec) {
                const int32_t d_vec = ((vec + vec_start) * kThreadGroupSize + threadIdx.x);
                smem_grad_sum[d_vec] = grad_sum[vec];
            }
        }
    }
}

    // clang-format on